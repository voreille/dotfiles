[["%",{"text":"train_model_2d.py","registerMode":0}],["\"",{"text":",","registerMode":1}],["1",{"text":"up_stack = [\n    upsample(512, 3),  # 4x4 -> 8x8\n    upsample(256, 3),  # 8x8 -> 16x16\n    upsample(128, 3),  # 16x16 -> 32x32\n    upsample(64, 3),  # 32x32 -> 64x64\n]\n\n","registerMode":1}],["/",{"text":"\\bangles\\b","registerMode":0}],["2",{"text":"base_model = tf.keras.applications.MobileNetV2(input_shape=[128, 128, 3],\n                                               include_top=False)\n\n# Use the activations of these layers\nlayer_names = [\n    'block_1_expand_relu',  # 64x64\n    'block_3_expand_relu',  # 32x32\n    'block_6_expand_relu',  # 16x16\n    'block_13_expand_relu',  # 8x8\n    'block_16_project',  # 4x4\n]\nlayers = [base_model.get_layer(name).output for name in layer_names]\n\n# Create the feature extraction model\ndown_stack = tf.keras.Model(inputs=base_model.input, outputs=layers)\n\ndown_stack.trainable = False\n\n","registerMode":1}],["3",{"text":"        steps_per_execution=None,","registerMode":2}],["4",{"text":"        run_eagerly=None,","registerMode":2}],["5",{"text":"        weighted_metrics=None,","registerMode":2}],["0",{"text":"from src.models.models_2d import unet_model","registerMode":2}],["6",{"text":"        loss_weights=None,","registerMode":2}],["7",{"text":"from src","registerMode":2}],["8",{"text":"\ndef dice_loss(y_true, y_pred, loss_type='jaccard', smooth=1.):\n\n    y_true_f = tf.cast(tf.reshape(y_true, [-1]), tf.float32)\n    y_pred_f = tf.cast(tf.reshape(y_pred, [-1]), tf.float32)\n\n    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n\n    if loss_type == 'jaccard':\n        union = tf.reduce_sum(tf.square(y_pred_f)) + tf.reduce_sum(\n            tf.square(y_true_f))\n\n    elif loss_type == 'sorensen':\n        union = tf.reduce_sum(y_pred_f) + tf.reduce_sum(y_true_f)\n\n    else:\n        raise ValueError(\"Unknown `loss_type`: %s\" % loss_type)\n\n    return (1 - (2. * intersection + smooth) / (union + smooth))\n\n\ndef soft_dice_loss(y_true, y_pred, smooth=1):\n    eps = K.epsilon()\n    y_pred = K.clip(y_pred, eps, 1 - eps)\n    return tf.reduce_mean(\n        1 - (2 * tf.reduce_sum(y_true * y_pred, axis=(1, 2, 3, 4)) + smooth) /\n        (tf.reduce_sum(y_true, axis=(1, 2, 3, 4)) +\n         tf.reduce_sum(y_pred, axis=(1, 2, 3, 4)) + smooth))\n\n","registerMode":1}],["9",{"text":"\ndef tf_focal_loss(prediction_tensor,\n                  target_tensor,\n                  weights=None,\n                  alpha=0.25,\n                  gamma=2):\n    sigmoid_p = tf.nn.sigmoid(prediction_tensor)\n    zeros = tf.zeros_like(sigmoid_p, dtype=sigmoid_p.dtype)\n\n    # For poitive prediction, only need consider front part loss, back part is 0;\n    # target_tensor > zeros <=> z=1, so poitive coefficient = z - p.\n    pos_p_sub = tf.where(target_tensor > zeros, target_tensor - sigmoid_p,\n                         zeros)\n\n    # For negative prediction, only need consider back part loss, front part is 0;\n    # target_tensor > zeros <=> z=1, so negative coefficient = 0.\n    neg_p_sub = tf.where(target_tensor > zeros, zeros, sigmoid_p)\n    per_entry_cross_ent = - alpha * (pos_p_sub ** gamma) * tf.math.log(tf.clip_by_value(sigmoid_p, 1e-8, 1.0)) \\\n                            - (1 - alpha) * (neg_p_sub ** gamma) * tf.math.log(tf.clip_by_value(1.0 - sigmoid_p, 1e-8, 1.0))\n    return tf.reduce_sum(per_entry_cross_ent)\n\n\ndef labels_to_one_hot(ground_truth, num_classes=1):\n    \"\"\"\n    Converts ground truth labels to one-hot, sparse tensors.\n    Used extensively in segmentation losses.\n    :param ground_truth: ground truth categorical labels (rank `N`)\n    :param num_classes: A scalar defining the depth of the one hot dimension\n        (see `depth` of `tf.one_hot`)\n    :return: one-hot sparse tf tensor\n        (rank `N+1`; new axis appended at the end)\n    \"\"\"\n    # read input/output shapes\n    if isinstance(num_classes, tf.Tensor):\n        num_classes_tf = tf.cast(num_classes, tf.int32)\n    else:\n        num_classes_tf = tf.constant(num_classes, tf.int32)\n    input_shape = tf.shape(ground_truth)\n    output_shape = tf.concat(\n        [input_shape, tf.reshape(num_classes_tf, (1, ))], 0)\n\n    if num_classes == 1:\n        # need a sparse representation?\n        return tf.reshape(ground_truth, output_shape)\n\n    # squeeze the spatial shape\n    ground_truth = tf.reshape(ground_truth, (-1, ))\n    # shape of squeezed output\n    dense_shape = tf.stack([tf.shape(ground_truth)[0], num_classes_tf], 0)\n\n    # create a rank-2 sparse tensor\n    ground_truth = tf.cast(ground_truth, tf.int64)\n    ids = tf.range(tf.cast(dense_shape[0], tf.int64), dtype=tf.int64)\n    ids = tf.stack([ids, ground_truth], axis=1)\n    one_hot = tf.SparseTensor(indices=ids,\n                              values=tf.ones_like(ground_truth,\n                                                  dtype=tf.float32),\n                              dense_shape=tf.cast(dense_shape, tf.int64))\n\n    # resume the spatial dims\n    one_hot = tf.sparse.reshape(one_hot, output_shape)\n    return one_hot\n\n\ndef undecided_loss(prediction, ground_truth, weight_map=None):\n    \"\"\"\n    :param prediction:\n    :param ground_truth:\n    :param weight_map:\n    :return:\n    \"\"\"\n    ratio_undecided = 1.0 / tf.cast(tf.shape(prediction)[-1], tf.float32)\n    res_undecided = tf.reciprocal(\n        tf.reduce_mean(tf.abs(prediction - ratio_undecided), -1) + 0.0001)\n    if weight_map is None:\n        return tf.reduce_mean(res_undecided)\n    else:\n        res_undecided = tf.Print(tf.cast(res_undecided, tf.float32), [\n            tf.shape(res_undecided),\n            tf.shape(weight_map),\n            tf.shape(res_undecided * weight_map)\n        ],\n                                 message='test_printshape_und')\n        return tf.reduce_sum(res_undecided * weight_map /\n                             tf.reduce_sum(weight_map))\n\n\ndef volume_enforcement(prediction,\n                       ground_truth,\n                       weight_map=None,\n                       eps=0.001,\n                       hard=False):\n    \"\"\"\n    Computing a volume enforcement loss to ensure that the obtained volumes are\n    close and avoid empty results when something is expected\n    :param prediction:\n    :param ground_truth: labels\n    :param weight_map: potential weight map to apply\n    :param eps: epsilon to use as regulariser\n    :return:\n    \"\"\"\n\n    prediction = tf.cast(prediction, tf.float32)\n    if len(ground_truth.shape) == len(prediction.shape):\n        ground_truth = ground_truth[..., -1]\n    one_hot = labels_to_one_hot(ground_truth, tf.shape(prediction)[-1])\n\n    gt_red = tf.sparse_reduce_sum(one_hot, 0)\n    pred_red = tf.reduce_sum(prediction, 0)\n    if hard:\n        pred_red = tf.sparse_reduce_sum(\n            labels_to_one_hot(tf.argmax(prediction, -1),\n                              tf.shape(prediction)[-1]), 0)\n\n    if weight_map is not None:\n        n_classes = prediction.shape[1].value\n        weight_map_nclasses = tf.tile(\n            tf.expand_dims(tf.reshape(weight_map, [-1]), 1), [1, n_classes])\n        gt_red = tf.sparse_reduce_sum(weight_map_nclasses * one_hot,\n                                      reduction_axes=[0])\n        pred_red = tf.reduce_sum(weight_map_nclasses * prediction, 0)\n\n    return tf.reduce_mean(\n        tf.sqrt(\n            tf.square((gt_red + eps) / (pred_red + eps) - (pred_red + eps) /\n                      (gt_red + eps))))\n\n\ndef volume_enforcement_fin(prediction,\n                           ground_truth,\n                           weight_map=None,\n                           eps=0.001):\n    \"\"\"\n    Computing a volume enforcement loss to ensure that the obtained volumes are\n     close and avoid empty results when something is expected\n    :param prediction:\n    :param ground_truth:\n    :param weight_map:\n    :param eps:\n    :return:\n    \"\"\"\n\n    prediction = tf.cast(prediction, tf.float32)\n    if len(ground_truth.shape) == len(prediction.shape):\n        ground_truth = ground_truth[..., -1]\n    one_hot = labels_to_one_hot(ground_truth, tf.shape(prediction)[-1])\n    gt_red = tf.sparse_reduce_sum(one_hot, 0)\n    pred_red = tf.sparse_reduce_sum(\n        labels_to_one_hot(tf.argmax(prediction, -1),\n                          tf.shape(prediction)[-1]), 0)\n\n    if weight_map is not None:\n        n_classes = prediction.shape[1].value\n        weight_map_nclasses = tf.tile(\n            tf.expand_dims(tf.reshape(weight_map, [-1]), 1), [1, n_classes])\n        gt_red = tf.sparse_reduce_sum(weight_map_nclasses * one_hot,\n                                      reduction_axes=[0])\n        pred_red = tf.sparse_reduce_sum(\n            labels_to_one_hot(tf.argmax(prediction, -1),\n                              tf.shape(prediction)[-1]) * weight_map_nclasses,\n            0)\n\n    return tf.reduce_mean(\n        tf.sqrt(\n            tf.square((gt_red + eps) / (pred_red + eps) - (pred_red + eps) /\n                      (gt_red + eps))))\n\n\ndef generalised_dice_loss(prediction,\n                          ground_truth,\n                          weight_map=None,\n                          type_weight='Square'):\n    \"\"\"\n    Function to calculate the Generalised Dice Loss defined in\n        Sudre, C. et. al. (2017) Generalised Dice overlap as a deep learning\n        loss function for highly unbalanced segmentations. DLMIA 2017\n    :param prediction: the logits\n    :param ground_truth: the segmentation ground truth\n    :param weight_map:\n    :param type_weight: type of weighting allowed between labels (choice\n        between Square (square of inverse of volume),\n        Simple (inverse of volume) and Uniform (no weighting))\n    :return: the loss\n    \"\"\"\n    prediction = tf.cast(prediction, tf.float32)\n    if len(ground_truth.shape) == len(prediction.shape):\n        ground_truth = ground_truth[..., -1]\n    one_hot = labels_to_one_hot(ground_truth, tf.shape(prediction)[-1])\n\n    if weight_map is not None:\n        num_classes = prediction.shape[1].value\n        # weight_map_nclasses = tf.reshape(\n        #     tf.tile(weight_map, [num_classes]), prediction.get_shape())\n        weight_map_nclasses = tf.tile(\n            tf.expand_dims(tf.reshape(weight_map, [-1]), 1), [1, num_classes])\n        ref_vol = tf.sparse_reduce_sum(weight_map_nclasses * one_hot,\n                                       reduction_axes=[0])\n\n        intersect = tf.sparse_reduce_sum(weight_map_nclasses * one_hot *\n                                         prediction,\n                                         reduction_axes=[0])\n        seg_vol = tf.reduce_sum(tf.multiply(weight_map_nclasses, prediction),\n                                0)\n    else:\n        ref_vol = tf.sparse_reduce_sum(one_hot, reduction_axes=[0])\n        intersect = tf.sparse_reduce_sum(one_hot * prediction,\n                                         reduction_axes=[0])\n        seg_vol = tf.reduce_sum(prediction, 0)\n    if type_weight == 'Square':\n        weights = tf.reciprocal(tf.square(ref_vol))\n    elif type_weight == 'Simple':\n        weights = tf.reciprocal(ref_vol)\n    elif type_weight == 'Uniform':\n        weights = tf.ones_like(ref_vol)\n    else:\n        raise ValueError(\"The variable type_weight \\\"{}\\\"\"\n                         \"is not defined.\".format(type_weight))\n    new_weights = tf.where(tf.is_inf(weights), tf.zeros_like(weights), weights)\n    weights = tf.where(tf.is_inf(weights),\n                       tf.ones_like(weights) * tf.reduce_max(new_weights),\n                       weights)\n    generalised_dice_numerator = \\\n        2 * tf.reduce_sum(tf.multiply(weights, intersect))\n    # generalised_dice_denominator = \\\n    #     tf.reduce_sum(tf.multiply(weights, seg_vol + ref_vol)) + 1e-6\n    generalised_dice_denominator = tf.reduce_sum(\n        tf.multiply(weights, tf.maximum(seg_vol + ref_vol, 1)))\n    generalised_dice_score = \\\n        generalised_dice_numerator / generalised_dice_denominator\n    generalised_dice_score = tf.where(tf.is_nan(generalised_dice_score), 1.0,\n                                      generalised_dice_score)\n    return 1 - generalised_dice_score\n\n\ndef dice_plus_xent_loss(prediction, ground_truth, weight_map=None):\n    \"\"\"\n    Function to calculate the loss used in https://arxiv.org/pdf/1809.10486.pdf,\n    no-new net, Isenseee et al (used to win the Medical Imaging Decathlon).\n    It is the sum of the cross-entropy and the Dice-loss.\n    :param prediction: the logits\n    :param ground_truth: the segmentation ground truth\n    :param weight_map:\n    :return: the loss (cross_entropy + Dice)\n    \"\"\"\n    num_classes = tf.shape(prediction)[-1]\n\n    prediction = tf.cast(prediction, tf.float32)\n    loss_xent = cross_entropy(prediction, ground_truth, weight_map=weight_map)\n\n    # Dice as according to the paper:\n    one_hot = labels_to_one_hot(ground_truth, num_classes=num_classes)\n    softmax_of_logits = tf.nn.softmax(prediction)\n\n    if weight_map is not None:\n        weight_map_nclasses = tf.tile(tf.reshape(weight_map, [-1, 1]),\n                                      [1, num_classes])\n        dice_numerator = 2.0 * tf.sparse_reduce_sum(\n            weight_map_nclasses * one_hot * softmax_of_logits,\n            reduction_axes=[0])\n        dice_denominator = \\\n            tf.reduce_sum(weight_map_nclasses * softmax_of_logits,\n                          reduction_indices=[0]) + \\\n            tf.sparse_reduce_sum(one_hot * weight_map_nclasses,\n                                 reduction_axes=[0])\n    else:\n        dice_numerator = 2.0 * tf.sparse_reduce_sum(\n            one_hot * softmax_of_logits, reduction_axes=[0])\n        dice_denominator = \\\n            tf.reduce_sum(softmax_of_logits, reduction_indices=[0]) + \\\n            tf.sparse_reduce_sum(one_hot, reduction_axes=[0])\n\n    epsilon = 0.00001\n    loss_dice = -(dice_numerator + epsilon) / (dice_denominator + epsilon)\n    dice_numerator = tf.Print(dice_denominator,\n                              [dice_numerator, dice_denominator, loss_dice])\n\n    return loss_dice + loss_xent\n\n\ndef sensitivity_specificity_loss(prediction,\n                                 ground_truth,\n                                 weight_map=None,\n                                 r=0.05):\n    \"\"\"\n    Function to calculate a multiple-ground_truth version of\n    the sensitivity-specificity loss defined in \"Deep Convolutional\n    Encoder Networks for Multiple Sclerosis Lesion Segmentation\",\n    Brosch et al, MICCAI 2015,\n    https://link.springer.com/chapter/10.1007/978-3-319-24574-4_1\n    error is the sum of r(specificity part) and (1-r)(sensitivity part)\n    :param prediction: the logits\n    :param ground_truth: segmentation ground_truth.\n    :param r: the 'sensitivity ratio'\n        (authors suggest values from 0.01-0.10 will have similar effects)\n    :return: the loss\n    \"\"\"\n    if weight_map is not None:\n        # raise NotImplementedError\n        tf.logging.warning('Weight map specified but not used.')\n\n    prediction = tf.cast(prediction, tf.float32)\n    one_hot = labels_to_one_hot(ground_truth, tf.shape(prediction)[-1])\n\n    one_hot = tf.sparse_tensor_to_dense(one_hot)\n    # value of unity everywhere except for the previous 'hot' locations\n    one_cold = 1 - one_hot\n\n    # chosen region may contain no voxels of a given label. Prevents nans.\n    epsilon = 1e-5\n\n    squared_error = tf.square(one_hot - prediction)\n    specificity_part = tf.reduce_sum(\n        squared_error * one_hot, 0) / \\\n                       (tf.reduce_sum(one_hot, 0) + epsilon)\n    sensitivity_part = \\\n        (tf.reduce_sum(tf.multiply(squared_error, one_cold), 0) /\n         (tf.reduce_sum(one_cold, 0) + epsilon))\n\n    return tf.reduce_sum(r * specificity_part + (1 - r) * sensitivity_part)\n\n\ndef cross_entropy(prediction, ground_truth, weight_map=None):\n    \"\"\"\n    Function to calculate the cross-entropy loss function\n    :param prediction: the logits (before softmax)\n    :param ground_truth: the segmentation ground truth\n    :param weight_map:\n    :return: the cross-entropy loss\n    \"\"\"\n    if len(ground_truth.shape) == len(prediction.shape):\n        ground_truth = ground_truth[..., -1]\n\n    # TODO trace this back:\n    ground_truth = tf.cast(ground_truth, tf.int32)\n\n    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n        logits=prediction, labels=ground_truth)\n\n    if weight_map is None:\n        return tf.reduce_mean(entropy)\n\n    weight_sum = tf.maximum(tf.reduce_sum(weight_map), 1e-6)\n    return tf.reduce_sum(entropy * weight_map / weight_sum)\n\n\ndef cross_entropy_dense(prediction, ground_truth, weight_map=None):\n    if weight_map is not None:\n        raise NotImplementedError\n\n    entropy = tf.nn.softmax_cross_entropy_with_logits(logits=prediction,\n                                                      labels=ground_truth)\n    return tf.reduce_mean(entropy)\n\n\ndef wasserstein_disagreement_map(prediction,\n                                 ground_truth,\n                                 weight_map=None,\n                                 M=None):\n    \"\"\"\n    Function to calculate the pixel-wise Wasserstein distance between the\n    flattened prediction and the flattened labels (ground_truth) with respect\n    to the distance matrix on the label space M.\n    :param prediction: the logits after softmax\n    :param ground_truth: segmentation ground_truth\n    :param M: distance matrix on the label space\n    :return: the pixelwise distance map (wass_dis_map)\n    \"\"\"\n    if weight_map is not None:\n        # raise NotImplementedError\n        tf.logging.warning('Weight map specified but not used.')\n\n    assert M is not None, \"Distance matrix is required.\"\n    # pixel-wise Wassertein distance (W) between flat_pred_proba and flat_labels\n    # wrt the distance matrix on the label space M\n    num_classes = prediction.shape[1].value\n    ground_truth.set_shape(prediction.shape)\n    unstack_labels = tf.unstack(ground_truth, axis=-1)\n    unstack_labels = tf.cast(unstack_labels, dtype=tf.float64)\n    unstack_pred = tf.unstack(prediction, axis=-1)\n    unstack_pred = tf.cast(unstack_pred, dtype=tf.float64)\n    # print(\"shape of M\", M.shape, \"unstacked labels\", unstack_labels,\n    #       \"unstacked pred\" ,unstack_pred)\n    # W is a weighting sum of all pairwise correlations (pred_ci x labels_cj)\n    pairwise_correlations = []\n    for i in range(num_classes):\n        for j in range(num_classes):\n            pairwise_correlations.append(\n                M[i, j] * tf.multiply(unstack_pred[i], unstack_labels[j]))\n    wass_dis_map = tf.add_n(pairwise_correlations)\n    return wass_dis_map\n\n\ndef generalised_wasserstein_dice_loss(prediction,\n                                      ground_truth,\n                                      weight_map=None):\n    \"\"\"\n    Function to calculate the Generalised Wasserstein Dice Loss defined in\n        Fidon, L. et. al. (2017) Generalised Wasserstein Dice Score\n        for Imbalanced Multi-class Segmentation using Holistic\n        Convolutional Networks.MICCAI 2017 (BrainLes)\n    :param prediction: the logits\n    :param ground_truth: the segmentation ground_truth\n    :param weight_map:\n    :return: the loss\n    \"\"\"\n    if weight_map is not None:\n        # raise NotImplementedError\n        tf.logging.warning('Weight map specified but not used.')\n\n    prediction = tf.cast(prediction, tf.float32)\n    num_classes = prediction.shape[1].value\n    one_hot = labels_to_one_hot(ground_truth, tf.shape(prediction)[-1])\n\n    one_hot = tf.sparse_tensor_to_dense(one_hot)\n    # M = tf.cast(M, dtype=tf.float64)\n    # compute disagreement map (delta)\n    M = M_tree\n    delta = wasserstein_disagreement_map(prediction, one_hot, M=M)\n    # compute generalisation of all error for multi-class seg\n    all_error = tf.reduce_sum(delta)\n    # compute generalisation of true positives for multi-class seg\n    one_hot = tf.cast(one_hot, dtype=tf.float64)\n    true_pos = tf.reduce_sum(tf.multiply(\n        tf.constant(M[0, :num_classes], dtype=tf.float64), one_hot),\n                             axis=1)\n    true_pos = tf.reduce_sum(tf.multiply(true_pos, 1. - delta), axis=0)\n    WGDL = 1. - (2. * true_pos) / (2. * true_pos + all_error)\n    return tf.cast(WGDL, dtype=tf.float32)\n\n\ndef dice(prediction, ground_truth, weight_map=None):\n    \"\"\"\n    Function to calculate the dice loss with the definition given in\n        Milletari, F., Navab, N., & Ahmadi, S. A. (2016)\n        V-net: Fully convolutional neural\n        networks for volumetric medical image segmentation. 3DV 2016\n    using a square in the denominator\n    :param prediction: the logits\n    :param ground_truth: the segmentation ground_truth\n    :param weight_map:\n    :return: the loss\n    \"\"\"\n    prediction = tf.cast(prediction, tf.float32)\n    if len(ground_truth.shape) == len(prediction.shape):\n        ground_truth = ground_truth[..., -1]\n    one_hot = labels_to_one_hot(ground_truth, tf.shape(prediction)[-1])\n\n    if weight_map is not None:\n        num_classes = prediction.shape[1].value\n        weight_map_nclasses = tf.tile(\n            tf.expand_dims(tf.reshape(weight_map, [-1]), 1), [1, num_classes])\n        dice_numerator = 2.0 * tf.sparse_reduce_sum(\n            weight_map_nclasses * one_hot * prediction, reduction_axes=[0])\n        dice_denominator = \\\n            tf.reduce_sum(weight_map_nclasses * tf.square(prediction),\n                          reduction_indices=[0]) + \\\n            tf.sparse_reduce_sum(one_hot * weight_map_nclasses,\n                                 reduction_axes=[0])\n    else:\n        dice_numerator = 2.0 * tf.sparse_reduce_sum(one_hot * prediction,\n                                                    reduction_axes=[0])\n        dice_denominator = \\\n            tf.reduce_sum(tf.square(prediction), reduction_indices=[0]) + \\\n            tf.sparse_reduce_sum(one_hot, reduction_axes=[0])\n    epsilon = 0.00001\n\n    dice_score = (dice_numerator + epsilon) / (dice_denominator + epsilon)\n    # dice_score.set_shape([num_classes])\n    # minimising (1 - dice_coefficients)\n    return 1.0 - tf.reduce_mean(dice_score)\n\n\ndef dice_nosquare(prediction, ground_truth, weight_map=None):\n    \"\"\"\n    Function to calculate the classical dice loss\n    :param prediction: the logits\n    :param ground_truth: the segmentation ground_truth\n    :param weight_map:\n    :return: the loss\n    \"\"\"\n    prediction = tf.cast(prediction, tf.float32)\n    if len(ground_truth.shape) == len(prediction.shape):\n        ground_truth = ground_truth[..., -1]\n    one_hot = labels_to_one_hot(ground_truth, tf.shape(prediction)[-1])\n\n    # dice\n    if weight_map is not None:\n        num_classes = prediction.shape[1].value\n        weight_map_nclasses = tf.tile(\n            tf.expand_dims(tf.reshape(weight_map, [-1]), 1), [1, num_classes])\n        dice_numerator = 2.0 * tf.sparse_reduce_sum(\n            weight_map_nclasses * one_hot * prediction, reduction_axes=[0])\n        dice_denominator = \\\n            tf.reduce_sum(prediction * weight_map_nclasses,\n                          reduction_indices=[0]) + \\\n            tf.sparse_reduce_sum(weight_map_nclasses * one_hot,\n                                 reduction_axes=[0])\n    else:\n        dice_numerator = 2.0 * tf.sparse_reduce_sum(one_hot * prediction,\n                                                    reduction_axes=[0])\n        dice_denominator = tf.reduce_sum(prediction, reduction_indices=[0]) + \\\n                           tf.sparse_reduce_sum(one_hot, reduction_axes=[0])\n    epsilon = 0.00001\n\n    dice_score = (dice_numerator + epsilon) / (dice_denominator + epsilon)\n    # dice_score.set_shape([num_classes])\n    # minimising (1 - dice_coefficients)\n    return 1.0 - tf.reduce_mean(dice_score)\n\n\ndef tversky(prediction, ground_truth, weight_map=None, alpha=0.5, beta=0.5):\n    \"\"\"\n    Function to calculate the Tversky loss for imbalanced data\n        Sadegh et al. (2017)\n        Tversky loss function for image segmentation\n        using 3D fully convolutional deep networks\n    :param prediction: the logits\n    :param ground_truth: the segmentation ground_truth\n    :param alpha: weight of false positives\n    :param beta: weight of false negatives\n    :param weight_map:\n    :return: the loss\n    \"\"\"\n    prediction = tf.to_float(prediction)\n    if len(ground_truth.shape) == len(prediction.shape):\n        ground_truth = ground_truth[..., -1]\n    one_hot = labels_to_one_hot(ground_truth, tf.shape(prediction)[-1])\n    one_hot = tf.sparse_tensor_to_dense(one_hot)\n\n    p0 = prediction\n    p1 = 1 - prediction\n    g0 = one_hot\n    g1 = 1 - one_hot\n\n    if weight_map is not None:\n        num_classes = prediction.shape[1].value\n        weight_map_flattened = tf.reshape(weight_map, [-1])\n        weight_map_expanded = tf.expand_dims(weight_map_flattened, 1)\n        weight_map_nclasses = tf.tile(weight_map_expanded, [1, num_classes])\n    else:\n        weight_map_nclasses = 1\n\n    tp = tf.reduce_sum(weight_map_nclasses * p0 * g0)\n    fp = alpha * tf.reduce_sum(weight_map_nclasses * p0 * g1)\n    fn = beta * tf.reduce_sum(weight_map_nclasses * p1 * g0)\n\n    EPSILON = 0.00001\n    numerator = tp\n    denominator = tp + fp + fn + EPSILON\n    score = numerator / denominator\n    return 1.0 - tf.reduce_mean(score)\n\n\ndef dice_dense(prediction, ground_truth, weight_map=None):\n    \"\"\"\n    Computing mean-class Dice similarity.\n    :param prediction: last dimension should have ``num_classes``\n    :param ground_truth: segmentation ground truth (encoded as a binary matrix)\n        last dimension should be ``num_classes``\n    :param weight_map:\n    :return: ``1.0 - mean(Dice similarity per class)``\n    \"\"\"\n\n    if weight_map is not None:\n        raise NotImplementedError\n    prediction = tf.cast(prediction, dtype=tf.float32)\n    ground_truth = tf.cast(ground_truth, dtype=tf.float32)\n    ground_truth = tf.reshape(ground_truth, prediction.shape)\n    # computing Dice over the spatial dimensions\n    reduce_axes = list(range(len(prediction.shape) - 1))\n    dice_numerator = 2.0 * tf.reduce_sum(prediction * ground_truth,\n                                         axis=reduce_axes)\n    dice_denominator = \\\n        tf.reduce_sum(tf.square(prediction), axis=reduce_axes) + \\\n        tf.reduce_sum(tf.square(ground_truth), axis=reduce_axes)\n\n    epsilon = 0.00001\n\n    dice_score = (dice_numerator + epsilon) / (dice_denominator + epsilon)\n    return 1.0 - tf.reduce_mean(dice_score)\n\n\ndef dice_dense_nosquare(prediction, ground_truth, weight_map=None):\n    \"\"\"\n    Computing mean-class Dice similarity with no square terms in the denominator\n    :param prediction: last dimension should have ``num_classes``\n    :param ground_truth: segmentation ground truth (encoded as a binary matrix)\n        last dimension should be ``num_classes``\n    :param weight_map:\n    :return: ``1.0 - mean(Dice similarity per class)``\n    \"\"\"\n\n    if weight_map is not None:\n        raise NotImplementedError\n    prediction = tf.cast(prediction, dtype=tf.float32)\n    ground_truth = tf.cast(ground_truth, dtype=tf.float32)\n    ground_truth = tf.reshape(ground_truth, prediction.shape)\n    # computing Dice over the spatial dimensions\n    reduce_axes = list(range(len(prediction.shape) - 1))\n    dice_numerator = 2.0 * tf.reduce_sum(prediction * ground_truth,\n                                         axis=reduce_axes)\n    dice_denominator = \\\n        tf.reduce_sum(prediction, axis=reduce_axes) + \\\n        tf.reduce_sum(ground_truth, axis=reduce_axes)\n    epsilon = 0.00001\n\n    dice_score = (dice_numerator + epsilon) / (dice_denominator + epsilon)\n    return 1.0 - tf.reduce_mean(dice_score)\n\n\ndef weighted_dice_coefficient(y_true, y_pred, axis=(1, 2, 3), smooth=0.00001):\n    \"\"\"\n    Weighted dice coefficient. Default axis assumes a \"channels first\" data structure\n    :param smooth:\n    :param y_true:\n    :param y_pred:\n    :param axis:\n    :return:\n    \"\"\"\n    return tf.keras.backend.mean(\n        2. * (tf.keras.backend.sum(y_true * y_pred, axis=axis) + smooth / 2) /\n        (tf.keras.backend.sum(y_true, axis=axis) +\n         tf.keras.backend.sum(y_pred, axis=axis) + smooth))\n\n\ndef weighted_dice_coefficient_loss(y_true, y_pred):\n    return -weighted_dice_coefficient(y_true, y_pred)","registerMode":2}],["-",{"text":",","registerMode":1}]]